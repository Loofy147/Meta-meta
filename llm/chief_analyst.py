"""Integrates a Large Language Model (LLM) to act as a "Chief Analyst."

This module leverages a powerful LLM, such as OpenAI's GPT models, to provide
qualitative, human-readable insights into the quantitative trading signals
generated by the system.

Its primary function is to take a structured, aggregated trading signal and
generate a detailed analysis and a tactical "playbook." This adds a crucial
layer of explainability and decision-support, bridging the gap between the raw
algorithmic output and human oversight. The output is structured as a JSON
object for easy integration into dashboards and other UIs.
"""

import os
import json
from openai import OpenAI
from dotenv import load_dotenv
from typing import Dict, Any

load_dotenv()

def get_openai_client() -> OpenAI:
    """Initializes and returns an authenticated OpenAI client.

    This function retrieves the OpenAI API key from the environment variables.
    It is a prerequisite for any communication with the OpenAI API.

    Returns:
        An authenticated `OpenAI` client instance.

    Raises:
        ValueError: If the `OPENAI_API_KEY` environment variable is not set.
    """
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("OPENAI_API_KEY environment variable not set. Cannot initialize LLM client.")
    return OpenAI(api_key=api_key)

def generate_analysis(signal: Dict[str, Any]) -> Dict[str, str]:
    """Generates an analysis and tactical playbook for a given trading signal.

    This function constructs a detailed, structured prompt that includes the
    asset, signal direction, confidence score, and the raw contributing signals.
    It then sends this prompt to the LLM and requests a response formatted as a
    JSON object with two keys: "analysis" and "playbook".

    Args:
        signal: The aggregated signal dictionary, which should contain details
            like asset, direction, confidence, and metadata.

    Returns:
        A dictionary containing the 'analysis' and 'playbook' as generated by
        the LLM. In case of a neutral signal or an error during the API call,
        it returns a default explanatory message.
    """
    if not signal or signal.get('direction') == 'hold':
        return {
            "analysis": "No significant trading signal was generated. Market conditions appear neutral or conflicting based on the available strategies.",
            "playbook": "No trading action is recommended at this time. It is best to wait for a clearer, more decisive signal before committing capital."
        }

    # Construct a detailed, structured prompt to guide the LLM's response.
    prompt = f"""
    **Trading Signal Analysis Request**

    **Asset:** {signal.get('asset', 'N/A')}
    **Aggregated Signal Direction:** {signal.get('direction', 'N/A').upper()}
    **Confidence Score:** {signal.get('confidence', 0.0):.2f}

    **Contributing Raw Signals:**
    ```json
    {json.dumps(signal.get('meta', {}).get('contributing_signals', []), indent=2)}
    ```

    **Your Task:**
    As the Chief Trading Analyst, provide an expert assessment of this signal.
    1.  **Analysis**: Write a concise analysis explaining the rationale behind the signal, synthesizing the contributing factors. Mention the convergence of indicators.
    2.  **Playbook**: Create a clear, tactical playbook for executing this trade. Include hypothetical entry points, take-profit levels, and stop-loss considerations based on standard technical analysis principles.

    **Format your entire response as a single, valid JSON object with exactly two keys: "analysis" and "playbook".**
    """

    try:
        client = get_openai_client()
        response = client.chat.completions.create(
            model="gpt-4-turbo",  # A powerful model suitable for this task
            messages=[
                {"role": "system", "content": "You are a Chief Trading Analyst for a top-tier quantitative hedge fund. Your insights are sharp, concise, and actionable."},
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"},  # Enforce JSON output
            temperature=0.4  # Lower temperature for more focused, deterministic output
        )

        # The API response content should be a JSON string, which we parse.
        analysis_json = json.loads(response.choices[0].message.content)
        return analysis_json

    except (ValueError, Exception) as e:
        print(f"An error occurred while communicating with the LLM API: {e}")
        return {
            "analysis": "An error occurred while generating the analysis. The LLM service may be unavailable or misconfigured.",
            "playbook": "Could not generate a tactical playbook due to an internal system error."
        }

if __name__ == '__main__':
    # Example Usage:
    # Demonstrates how to use the generate_analysis function with a mock signal.
    # Requires a valid OPENAI_API_KEY in the .env file to run.

    print("--- LLM Chief Analyst Example ---")
    mock_signal = {
        "signal_id": "a1b2c3d4-e5f6-7890-1234-567890abcdef",
        "asset": "BTC/USDT",
        "direction": "buy",
        "confidence": 0.85,
        "meta": {
            "contributing_signals": [
                {"strategy": "rsi", "direction": "buy", "confidence": 0.7, "details": "RSI crossed below 30"},
                {"strategy": "macd", "direction": "buy", "confidence": 0.6, "details": "MACD line crossed above signal line"},
                {"strategy": "sentiment", "direction": "buy", "confidence": 0.9, "details": "Positive news sentiment detected"}
            ]
        }
    }

    try:
        analysis = generate_analysis(mock_signal)
        print("\nGenerated Analysis:")
        print(json.dumps(analysis, indent=4))
    except ValueError as e:
        print(f"\nError: {e}")
