"""
LLM Chief Analyst

This module integrates with a large language model (LLM), such as OpenAI's GPT,
to act as a "Chief Analyst." It takes structured trading signals as input and
generates qualitative, human-readable analysis and tactical playbooks.

Its role is to provide a layer of interpretation and explainability on top of the
quantitative signals produced by the system, bridging the gap between raw data
and human decision-making.
"""

import os
import json
from openai import OpenAI
from dotenv import load_dotenv
from typing import Dict, Any

load_dotenv()

def get_openai_client() -> OpenAI:
    """
    Initializes and returns the OpenAI client.

    This function ensures that the necessary API key is configured in the
    environment variables.

    Returns:
        OpenAI: An authenticated OpenAI client instance.

    Raises:
        ValueError: If the OPENAI_API_KEY environment variable is not set.
    """
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("OPENAI_API_KEY environment variable not set. Cannot initialize LLM client.")
    return OpenAI(api_key=api_key)

def generate_analysis(signal: Dict[str, Any]) -> Dict[str, str]:
    """
    Generates a human-readable analysis and tactical playbook for a given signal.

    This function constructs a detailed prompt based on the aggregated signal,
    sends it to the configured LLM, and parses the structured JSON response.

    Args:
        signal (Dict[str, Any]): The aggregated signal dictionary, containing
                                 asset, direction, confidence, and metadata.

    Returns:
        Dict[str, str]: A dictionary containing the 'analysis' and 'playbook'
                        generated by the LLM. In case of an error, default
                        messages are returned.
    """
    if not signal or signal.get('direction') == 'hold':
        return {
            "analysis": "No significant trading signal detected. Market conditions are neutral or conflicting.",
            "playbook": "No action is recommended at this time. Continue to monitor for clearer opportunities."
        }

    # Construct a detailed, structured prompt for the LLM
    prompt = f"""
    **Trading Signal Analysis Request**

    **Asset:** {signal.get('asset', 'N/A')}
    **Aggregated Signal Direction:** {signal.get('direction', 'N/A').upper()}
    **Confidence Score:** {signal.get('confidence', 0.0):.2f}

    **Contributing Signals:**
    {json.dumps(signal.get('meta', {}).get('contributing_signals', []), indent=2)}

    **Task:**
    1.  **Provide a concise, expert-level analysis** of this trading signal, considering the contributing factors.
    2.  **Generate a clear, tactical playbook** for a trader executing this signal. Include potential entry points, take-profit levels, and stop-loss considerations.

    **Format your response as a JSON object with two keys: "analysis" and "playbook".**
    """

    try:
        client = get_openai_client()
        response = client.chat.completions.create(
            model="gpt-4-turbo",
            messages=[
                {"role": "system", "content": "You are a Chief Trading Analyst for a sophisticated quantitative trading firm. Provide clear, concise, and actionable insights."},
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"},
            temperature=0.5 # Encourage more deterministic responses
        )

        analysis_json = json.loads(response.choices[0].message.content)
        return analysis_json

    except (ValueError, Exception) as e:
        print(f"An error occurred while communicating with the LLM: {e}")
        return {
            "analysis": "An error occurred while generating the analysis. The LLM may be unavailable or misconfigured.",
            "playbook": "Could not generate a playbook due to an internal error."
        }

if __name__ == '__main__':
    # Example Usage:
    # Demonstrates how to use the generate_analysis function with a mock signal.
    # Requires a valid OPENAI_API_KEY in the .env file to run.

    print("--- LLM Chief Analyst Example ---")
    mock_signal = {
        "signal_id": "a1b2c3d4-e5f6-7890-1234-567890abcdef",
        "asset": "BTC/USDT",
        "direction": "buy",
        "confidence": 0.85,
        "meta": {
            "contributing_signals": [
                {"strategy": "rsi", "direction": "buy", "confidence": 0.7, "details": "RSI crossed below 30"},
                {"strategy": "macd", "direction": "buy", "confidence": 0.6, "details": "MACD line crossed above signal line"},
                {"strategy": "sentiment", "direction": "buy", "confidence": 0.9, "details": "Positive news sentiment detected"}
            ]
        }
    }

    try:
        analysis = generate_analysis(mock_signal)
        print("\nGenerated Analysis:")
        print(json.dumps(analysis, indent=4))
    except ValueError as e:
        print(f"\nError: {e}")
